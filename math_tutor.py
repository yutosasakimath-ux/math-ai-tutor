import streamlit as st
import google.generativeai as genai

# --- 1. ã‚¢ãƒ—ãƒªã®åˆæœŸè¨­å®š ---
st.set_page_config(page_title="æ•°å­¦AIãƒãƒ¥ãƒ¼ã‚¿ãƒ¼", page_icon="ğŸ“")

st.title("ğŸ“ é«˜æ ¡æ•°å­¦ AIãƒãƒ¥ãƒ¼ã‚¿ãƒ¼")
st.caption("ã‚ã‹ã‚‰ãªã„å•é¡Œã‚’è³ªå•ã—ã¦ã¿ã‚ˆã†ã€‚ãƒ’ãƒ³ãƒˆã‚’å‡ºã—ã¦ä¸€ç·’ã«è€ƒãˆã¦ãã‚Œã‚‹ã‚ˆï¼")

# --- 2. ä¼šè©±å±¥æ­´ã®ä¿å­˜å ´æ‰€ã‚’ä½œã‚‹ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 3. ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
with st.sidebar:
    st.header("å…ˆç”Ÿç”¨ç®¡ç†ç”»é¢")
    
    # ã€å¤‰æ›´ç‚¹ã€‘APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
    # ã‚µãƒ¼ãƒãƒ¼ã«ã‚­ãƒ¼ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã„ã€ãªã‘ã‚Œã°æ‰‹å‹•å…¥åŠ›æ¬„ã‚’å‡ºã™
    if "GEMINI_API_KEY" in st.secrets:
        api_key = st.secrets["GEMINI_API_KEY"]
        st.success("âœ… èªè¨¼æ¸ˆã¿ï¼ˆã‚µãƒ¼ãƒãƒ¼ã®ã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼‰")
    else:
        api_key = st.text_input("Gemini APIã‚­ãƒ¼ã‚’å…¥åŠ›", type="password")
    
    st.markdown("---")
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆAIã¸ã®æŒ‡ç¤ºæ›¸ï¼‰
    system_instruction = """
    ã‚ãªãŸã¯æ—¥æœ¬ã®é«˜æ ¡ã®è¦ªåˆ‡ã§å„ªç§€ãªæ•°å­¦æ•™å¸«ã§ã™ã€‚
    ç”Ÿå¾’ã‹ã‚‰ã®æ•°å­¦ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    
    ã€æŒ‡å°ã®ãƒ«ãƒ¼ãƒ«ã€‘
    1. **ã™ãã«æœ€çµ‚çš„ãªæ­£è§£ã‚’æ•™ãˆãªã„ã“ã¨**ã€‚
    2. ç”Ÿå¾’ãŒè‡ªåŠ›ã§è§£ã‘ã‚‹ã‚ˆã†ã«ã€æ®µéšçš„ãªãƒ’ãƒ³ãƒˆã‚„ã€è€ƒãˆæ–¹ã®é“ç­‹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
    3. ç”Ÿå¾’ãŒé–“é•ãˆã¦ã„ã‚‹å ´åˆã¯ã€å¦å®šã›ãšã€Œæƒœã—ã„ï¼ã€ã€Œã“ã“ã‚’ç¢ºèªã—ã¦ã¿ã¦ã€ã¨åŠ±ã¾ã—ã¦ãã ã•ã„ã€‚
    4. æ•°å¼ã¯LaTeXå½¢å¼ï¼ˆ$ãƒãƒ¼ã‚¯ã§å›²ã‚€ï¼‰ã‚’ä½¿ã£ã¦ç¶ºéº—ã«è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚
    5. è§£èª¬ã¯é«˜æ ¡ç”Ÿã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„å¹³æ˜“ãªè¨€è‘‰ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
    """

# --- 4. ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
if api_key:
    genai.configure(api_key=api_key)
    
    # ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«è‡ªå‹•é¸æŠãƒ­ã‚¸ãƒƒã‚¯
    try:
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
        
        if available_models:
            # å„ªå…ˆé †ä½ï¼šFlash -> Pro -> ãã®ä»–
            priority_keywords = ["flash", "pro", "gemini-1.5", "gemini-1.0"]
            selected_model_name = available_models[0]
            
            for keyword in priority_keywords:
                found = next((m for m in available_models if keyword in m), None)
                if found:
                    selected_model_name = found
                    break
            
            # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
            model = genai.GenerativeModel(
                model_name=selected_model_name,
                system_instruction=system_instruction
            )
        else:
            st.error("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

# --- 5. éå»ã®ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 6. æ–°ã—ã„è³ªå•ã®å‡¦ç† ---
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©ã£ã¦ä½•ï¼Ÿï¼‰"):
    if not api_key:
        st.warning("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«APIã‚­ãƒ¼ã‚’å…¥ã‚Œã¦ãã ã•ã„")
        st.stop()

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    # å±¥æ­´ã«è¿½åŠ ï¼ˆä¿å­˜ï¼‰
    st.session_state.messages.append({"role": "user", "content": prompt})

    # AIã®å›ç­”ã‚’ç”Ÿæˆ
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        try:
            # éå»ã®ä¼šè©±å±¥æ­´ã‚’AIã«æ¸¡ã™å½¢ã«å¤‰æ›
            chat_history_for_ai = [
                {"role": m["role"], "parts": [m["content"]]} 
                for m in st.session_state.messages 
                if m["role"] != "system" # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯é™¤å¤–
            ]
            
            # ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
            chat = model.start_chat(history=chat_history_for_ai)
            response = chat.send_message(prompt, stream=True)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response)
            
            # AIã®å›ç­”ã‚‚å±¥æ­´ã«è¿½åŠ ï¼ˆä¿å­˜ï¼‰
            st.session_state.messages.append({"role": "model", "content": full_response})

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")